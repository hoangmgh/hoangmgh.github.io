---
layout: post
title:  Sparse-Mixture Gaussian clustering for genetic demultiplexing
date: 2023-10-16 
description: An introduction to souporcell clustering with the mixture gaussian model
tags: math genetic-demultiplexing
categories: Bioinformatics
related_posts: false
featured: true
thumbnail: /assets/img/souporcell.png

---

I attempted to analyze  the source code of [souporcell](https://github.com/wheaton5/souporcell) and finally was able to understand its singlet identification step [souporcell.py](https://github.com/wheaton5/souporcell/blob/master/souporcell_pipeline.py). From my experience, the source code is much more straightforward to re-engineer than the mathematical formula given in the published paper, which might be a bit confusing. 


At the very core, souporcell attempts to model the genotype of each cell as a vector $$v_c$$ of size $$1 \times l $$, whose entry is calculated as 
\begin{equation}
    v_{cl} =\frac{r_{cl}}{a_{cl}+r_{cl}}    
\end{equation}


That is, we represented a cell's genotype by a vector, whose entries contains the fraction of reads that support the reference loci. If we have $$8$$ reads mapped to a loci $$l$$, where $$3$$ reads show evidence for the reference genotype and $$5$$ for the alternative, this value is $$3/8$$, for example. The entries of this vector can take in the value of $$0$$, $$0.5$$, or $$1$$, corresponding to the $$0/0$$, $$0/1$$ and $$1/1$$ genotype. However, due to noise and coverage, the value can be skewed away. 


**1. Mixture Gaussian model**

Assuming that each cell has its genotype data generated by vartrix, we can generate the vector for each cell by using the **ref.mtx** and the **alt.mtx** files given by souporcell. Now, the problem of assigning a cell an identity is equated to clustering the given cells into $$K$$ group. Since single-cell data is too sparse, any methods that measure distance between cells might not be very effective. Moreover, we can think of the set of vectors $$v_c$$ will cluster into $$K$$ group, corresponding to $$K$$ donors, and the center of these $$K$$ groups will be the donor's genotypes. We can use the Mixture-Gaussian method to solve this problem.  

Assuming that each cell (observation) is independently drawn from a mixture of $$K$$ components, then the probability of seeing the vector $$v_c$$ is:

\begin{equation}
    P(v_c)= \sum_{i=1}^8 \pi_i N( \theta_i| K=i) = \sum_{i=1}^8 P(K=i) N(v=v_c |\theta_i , K=i)
\end{equation}

Here we can think of this as a two-fold process: for each component $$k$$, we pick it with the probability $$P(K=i)$$. Each component is "tagged" with a probability distribution. In our case, the distribution is Multivariate-Gaussian, so it is given by $$N(v=v_c ,\theta_i)$$. A Multivariate-Gaussian take in two parameters: the center vector $$\theta_i$$ and a covariance matrix $$\Sigma$$. For simplicity, I think the author assume that most genes are pair-wise equivariant. The probability of "picking" the components must sum up to $$1$$, and hence $$P(K=i)$$ represents the weight of the component. Here, the author also assumes that the probability of drawing from each component is equal, so we can drop the terms $$P(K=i)$$, and our probability $$P(v_c)$$ can be thought of as an averaging of $$K$$ probabilities. 

The formula for $$N(v=v_c \| \theta_i)$$ is also well-defined:

\begin{equation}
    N(v=v_c \| \theta_i ,K=i) = e^{-\frac{1}{2}\|v_c-\theta_i\|_2 }
\end{equation}

    
The further your vector $$v_c$$ from the centre $$\theta_i$$, the smaller the $$l_2$$ distance, and hence the bigger the probability.s When the distance is large enough, the exponent approach $$-\infty$$, and hence the probability approaches $$e^{-\infty} \longrightarrow 0$$ 

**2. Sparse implementation**


Now, it is not very straightforward to calculate $$\|v_c-\theta_i\|$$, given that not all loci of cell are observed. That is, most of entries of $$v_c$$ wont have values, due to the fact that 
we don't see any reads mapped to the loci ($$r_{cl}=a_{cl}=0$$) at loci $$l$$. To deal with this situation, the author proposed the "sparse" mixture implementation.

Now, we can substitute the distance $$\|v_c-\theta_i\|$$ with a version where only observed values contribute to the distance. To find the set of $$\theta_1$$, $$\theta_2$$,...$$\theta_K$$, we use the 
Gradient-descent method to optimize our loss function. 

Note that the problem is now in fact an **optimization** problem, where a good set of parameters $$\theta_i$$ is the one that maximize the probability of seeing the independent observations. Hence, we will be finding
the centre vectors such that the following is maximized:
\begin{equation}
   l(\theta_1,\theta_2,...,\theta_K)= \Pi_{i=1}^c P(v_i \| \theta_1,\theta_2,..,\theta_K)
\end{equation}
which is also equivalent to maximizing the log (hence we call it the log-loss function)
\begin{equation}
    log l(\theta)= log P(v_1 )+...+ log P(v_2)
\end{equation}


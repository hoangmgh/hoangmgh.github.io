---
layout: post
title:  ðŸ“¢ Principle Component Analysis (PCA) - intuition
date: 2022-10-04
description: Learning PCA from the perspective of a low-rank approximation
tags: math single-cell
categories: Bioinformatics
related_posts: false
featured: true
thumbnail: /assets/img/pca_thumb.png
---

I think most scientists who have the privilege to work with single-cell RNA-seq data tend to overlook the concept of PCA. However, PCA is an essential step in  of our single-cell dimension reduction and clustering pipeline. When I first learn about the Principle Component Analysis, I was taught from the perspective of a linear "transformation" of the original data, e.g. scaling and rotation, in such a way that the resulting matrix have  **maximized variance**. This was also taught using data with only 3 features and k=2 components -  obviously because it is easy to visualize in 2D space. While this definition is useful and easy to compute for new-comers, honestly , I think it is not intuitive enough and doesn't do justice to  the whole purpose of PCA. In reality, for very high dimensional data, the 2D intuition won't help us generalize to n-dimensions. 

I would like to explain this concept formally  in the perspective of a low-rank approximation problem. PCA can be characterized as finding a low-dimensional "box" that can "fit" our matrix, which  currently live in a  high dimension, e.g. 30,000 features. This is only plausible as we assume there is some **inherent structure** in our data that is low dimensional. Specifically, what is "low" here is the **rank** of the data. 


**1. Rank of a matrix**

A linear combination of vectors $$v_1$$, $$v_2$$,... $$v_k$$ is simply defined as a weighted sum of the form
\begin{align}
a_1 v_1 + a_2 v_2 + ... + a_k v_k =  [\vec{v_1} \\ \vec{v_2} \\ ... \\  \vec{v_n}] 
\end{align}

A collection of vectors $$v_1$$, $$v_2$$,.... $$v_k$$ is linearly independent if there is no linear combination of them that produce the zero vector, except for the trivial 0-weighted linear combination. 

The rank of a matrix $$A$$ is the size of the largest subset of $$A$$'s columns which are linearly independent. If $$rank(A)=r$$, then  if one picks $$r+1$$ random columns, they are guaranteed to be **linearly dependent**. There is also another way to define the rank. We define the column space of a matrix as the set of all possible linear combination of its columns. A basis for the column space is a linear independent collection of elements of the column space of the largest possible size. Given this, every element of the column space can be represented as a linear combination of the elements in a basis. If we define the basis to be a matrix $$B$$, then we can write $$A$$ as:
\begin{equation}
   A = BC = [\vec{b_1} \\ \vec{b_2} \\ ... \\ \vec{b_r}]
\end{equation}
where $$a_i$$ are of size $$\mathbb{R^{r}}$$.  

**2. Low-Rank Approximation**

In real-world, the data are not always low-rank, but **approximately** low rank. This means we can find a substituting matrix $$\overline{A}$$ of low  rank so that
\begin{equation}
E = A - \overline{A} 
\end{equation}
is small by some sense. What does this mean? and how should we find an obscured $$\overline{A}$$ ? In simple terms, we can think of projecting $$A$$ onto a subspace $$S$$ of rank $$k$$. To be more precise,
we are looking for a subspace S spanned by the basis $$V=(w_1 w_2 ... w_k) $$ such that 
$$\overline{A} = proj_S (A) \in span(V)$$
As always, we can define this projection in terms of a projection matrix $$V$$:
\begin{equation}
    \overline{A} = VV^T (A) = [VV^Ta_1 VV^Ta_2....VV^Ta_n]
\end{equation}
what we are doing here is in fact projecting each column vector of $$A$$, i.e. $$a_i$$, onto the subspace spanned by $$V$$. To minimize the distance between $A$ and its projection is very similar to jointly
minimimze the distance between each column and their projection. Specifically, we are trying to minimize with the euclidean distance the following:
$$f(A) = \sum_{i=1}^k \|VV^T a_i - a_i\| $$ 

If we implement singular-value decomposion on $$A$$ and obtain $$A=U\sum V^T$$, then a well-known result from Ekart-Young-Mirsky theorem is that $$U_l$$ is among a bunch of $$W$$s that can minimize
$$f(A)$$, where we define $$U_l$$ to be the first $$l$$ columns of $$U$$ obtained from the SVD of $$A$$. Moreover, we also know that vector $$V$$ is consisted of the eigenvectors of $$AA^T$$:
\begin{equation}
    AA^T = V\sum{ }^2V^T 
\end{equation}

In summary, we know that principle components of a given matrix is the **orthogonal projection** of the input data into a subspace spanned by the first $$l$$ columns of the singular matrix $S$.

<div class="row" style="text-align: center">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="/assets/img/pca_thumb.png" title="Lineage dotplot" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    An example of projecting 2D data onto a subspace spanned by 1D vector
</div>
<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>üì¢ Principle Component Analysis (PCA) - intuition | Hoang (Anh) Tran</title> <meta name="author" content="Hoang (Anh) Tran"> <meta name="description" content="Learning PCA from the perspective of a low-rank approximation"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hoangmgh.github.io/blog/2022/PCA/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script async src="https://www.googletagmanager.com/gtag/js?id={{%20site.google_analytics%20}}"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1W7V2Y0MXW");</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Hoang¬†</span>(Anh)¬†Tran</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Bioinformatics<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Arts</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">üì¢ Principle Component Analysis (PCA) - intuition</h1> <p class="post-meta">October 4, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a> ¬† ¬∑ ¬† <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a> ¬† <a href="/blog/tag/single-cell"> <i class="fa-solid fa-hashtag fa-sm"></i> single-cell</a> ¬† ¬† ¬∑ ¬† <a href="/blog/category/bioinformatics"> <i class="fa-solid fa-tag fa-sm"></i> Bioinformatics</a> ¬† </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I think most scientists who have the privilege to work with single-cell RNA-seq data tend to overlook the concept of PCA. However, PCA is an essential step in of our single-cell dimension reduction and clustering pipeline. When I first learn about the Principle Component Analysis, I was taught from the perspective of a linear ‚Äútransformation‚Äù of the original data, e.g. scaling and rotation, in such a way that the resulting matrix have <strong>maximized variance</strong>. This was also taught using data with only 3 features and k=2 components - obviously because it is easy to visualize in 2D space. While this definition is useful and easy to compute for new-comers, honestly , I think it is not intuitive enough and doesn‚Äôt do justice to the whole purpose of PCA. In reality, for very high dimensional data, the 2D intuition won‚Äôt help us generalize to n-dimensions.</p> <p>I would like to explain this concept formally in the perspective of a low-rank approximation problem. PCA can be characterized as finding a low-dimensional ‚Äúbox‚Äù that can ‚Äúfit‚Äù our matrix, which currently live in a high dimension, e.g. 30,000 features. This is only plausible as we assume there is some <strong>inherent structure</strong> in our data that is low dimensional. Specifically, what is ‚Äúlow‚Äù here is the <strong>rank</strong> of the data.</p> <p><strong>1. Rank of a matrix</strong></p> <p>A linear combination of vectors \(v_1\), \(v_2\),‚Ä¶ \(v_k\) is simply defined as a weighted sum of the form \begin{align} a_1 v_1 + a_2 v_2 + ‚Ä¶ + a_k v_k = [\vec{v_1} \ \vec{v_2} \ ‚Ä¶ \ \vec{v_n}] \end{align}</p> <p>A collection of vectors \(v_1\), \(v_2\),‚Ä¶. \(v_k\) is linearly independent if there is no linear combination of them that produce the zero vector, except for the trivial 0-weighted linear combination.</p> <p>The rank of a matrix \(A\) is the size of the largest subset of \(A\)‚Äôs columns which are linearly independent. If \(rank(A)=r\), then if one picks \(r+1\) random columns, they are guaranteed to be <strong>linearly dependent</strong>. There is also another way to define the rank. We define the column space of a matrix as the set of all possible linear combination of its columns. A basis for the column space is a linear independent collection of elements of the column space of the largest possible size. Given this, every element of the column space can be represented as a linear combination of the elements in a basis. If we define the basis to be a matrix \(B\), then we can write \(A\) as: \begin{equation} A = BC = [\vec{b_1} \ \vec{b_2} \ ‚Ä¶ \ \vec{b_r}] \end{equation} where \(a_i\) are of size \(\mathbb{R^{r}}\).</p> <p><strong>2. Low-Rank Approximation</strong></p> <p>In real-world, the data are not always low-rank, but <strong>approximately</strong> low rank. This means we can find a substituting matrix \(\overline{A}\) of low rank so that \begin{equation} E = A - \overline{A} \end{equation} is small by some sense. What does this mean? and how should we find an obscured \(\overline{A}\) ? In simple terms, we can think of \(A\) as a projection onto a subspace \(S\) of rank \(k\). To be more precise, we are looking for a subspace S spanned by the basis \(V=(w_1 w_2 ... w_k)\) such that \(\overline{A} = proj_S (A) \in span(V)\) As always, we can define this projection in terms of a projection matrix \(V\): \begin{equation} \overline{A} = VV^T (A) = [VV^Ta_1 VV^Ta_2‚Ä¶.VV^Ta_n] \end{equation} what we are doing here is in fact projecting each column vector of \(A\), i.e. \(a_i\), onto the subspace spanned by \(V\). To minimize the distance between $A$ and its projection is very similar to jointly minimimze the distance between each column and their projection. Specifically, we are trying to minimize with the euclidean distance the following: \(f(A) = \sum_{i=1}^k \|VV^T a_i - a_i\|\)</p> <p>Thus, the PCA problem is in fact finding a low-rank subspace that best fit the data, where the data here would be the columns of a given matrix \(A\).</p> <p>If we implement singular-value decomposion on \(A\) and obtain \(A=USV^T\), then a well-known result from Ekart-Young-Mirsky theorem is that the first \(l\) columns of \(U\) is <strong>one of the solutions</strong> that minimize \(f(A)\). Moreover, we also know that vector \(V\) is consisted of the eigenvectors of \(AA^T\): \begin{equation} AA^T = VS^2V^T \end{equation}</p> <p>In summary, we conclude that principle components of a given matrix is the <strong>orthogonal projection</strong> of the input data into a subspace spanned by the first \(l\) columns of the singular matrix $S$.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pca_thumb-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pca_thumb-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pca_thumb-1400.webp"></source> <img src="/assets/img/pca_thumb.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Lineage dotplot" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An example of projecting 2D data onto a subspace spanned by 1D vector </div> <p><strong>3.Interpreting the result of PCA</strong></p> <p>Now that PCA can be intuitively explained by low-rank approximation, we are often burdened to interpret the result of PCA. Most courses teach PCA in the case of a linear combination. Indeed, with $U$ selected above becomes the new orthonormal projection, the new coordinates of this subspace is given by \(U^T A\) due to the fact that the new projection of \(A\) is \(UU^TA\). By the Ekart-Young-Mirsky theorem, the projection is also given by \(\overline{A} = U S_l V_l\) where \(S_l\) store the first \(l\) eigenvalues of \(A\), and \(V_l\) is the first \(l\) rows of \(V\).</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2024 Hoang (Anh) Tran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1W7V2Y0MXW"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1W7V2Y0MXW");</script> <script async src="https://rum.cronitor.io/script.js"></script> <script>window.cronitor=window.cronitor||function(){(window.cronitor.q=window.cronitor.q||[]).push(arguments)},cronitor("config",{clientKey:""});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>📢 📏 Attention-based Transformer, a mathematical primer | Hoang (Anh) Tran</title> <meta name="author" content="Hoang (Anh) Tran"> <meta name="description" content="Learning the mathematical assumption behind the SoA Deep Learning model"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hoangmgh.github.io/blog/2021/attention/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script async src="https://www.googletagmanager.com/gtag/js?id={{%20site.google_analytics%20}}"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1W7V2Y0MXW");</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Hoang </span>(Anh) Tran</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Bioinformatics<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Arts</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">📢 📏 Attention-based Transformer, a mathematical primer</h1> <p class="post-meta">November 6, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   <a href="/blog/tag/genetic-demultiplexing"> <i class="fa-solid fa-hashtag fa-sm"></i> genetic-demultiplexing</a>     ·   <a href="/blog/category/bioinformatics"> <i class="fa-solid fa-tag fa-sm"></i> Bioinformatics</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>There are a lot of tutorial on the net that teach the how and why of the dot-product attention Transformer model, such as that from <a href="https://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener" target="_blank">jalammar.github.io</a>. This is useful if you want to understand transformer by deriving the input, output, intermediate objects through matrix multiplication. While this is absolutely useful, most tutorials out there can be very confusing, and doesn’t really paint the big picture of attention mechanism. At the end of the day, I think one has to seperate the attention mechanism from transformer, in which attention is the <strong>engine</strong>. Most tutorials out there are also confusing matrix embedding with the attention mechanism itself!</p> <p>At the very core, formally speaking, the attention mechanism can be summarized by Bahdanau’s equation:</p> <p>\begin{equation} Attention(q,D) = \sum_{i=1}^{m} \alpha (q,k_i) v_i \end{equation} where we define \(D\) to be a “database” or dictionary of \(m\) tuples associating keys to values. Moreover, we denote \(q\) a query. Here we also have \(\alpha(q,k_i) \in \mathbb{R}\) to be a kind of mapping, which take in \(q\) and \(k_i\) and “spit out” attention weights. The whole process can be viewed as attention pooling. To make it simpler to understand, let’s consider the simple case where exactly one of the weights is \(1\) while the rest are \(0\). This is very similar to traditional database query, where only one key \(k_i\) are matched for a given query \(q\). In this case Attention function will “spit out” the matched value, i.e. \(v_i\). Thus, we basically map \(q\) to \(v_i\). If all weights are equal, this is basically averaging across all database (average pooling). In most case, some of the weights are diminished, while only a few are positive weights and contribute to the output. We can also normalize the weights to sum up to \(1\) by dividing their sum: \begin{equation} \alpha(q,k_i) = \frac{\alpha(q,k_i)}{\sum_{i=1}^m \alpha (q,k_j)} \end{equation}</p> <p>In deep learning setting, what we are trying to learn form the data is a set of attention weights, associating a given set of query \(q\)s to all keys \(k_i\).</p> <p>In the case of self-attention, the queries and the values are the same set. In a way, you are trying to infer some kind of correlation/cooccurence between queries and values from the weights, where the co-occurence weight matrix might not be necessarily symmetric.</p> <p>Most tutorials out there teaches the concept of self-attention from query, key, value matrices embedding. In the case of self-attention, you are trying to embed the same input matrices of \(n\) observations into three matrices. If we are start with \(n\) observations stored in matrix \(X\) of the size \(\mathbb{R^{n \times f}}\), then by multiplying them with \(Q,K,V \in \mathbb{R^{f \times d}}\),we arrive at \(Q = XW_Q\), \(K=XW_K\), and \(V=XW_V\), respectively. What this is doing is abstracting \(n\) initial observations as a set of \(n\) queries (in \(Q\)), \(n\) keys in \(K\), and \(n\) values in \(V\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/attention_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/attention_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/attention_2-1400.webp"></source> <img src="/assets/img/attention_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Starting from one input matrix X, one arrives at 3 different representation Q,K, and V. </div> <p>Thus, from the same set of observations, we are representing them differently. However, to tie this back to our Bahdanau’s equation, we “bulk” calculate the attention matrix by doing \begin{equation} attention(Q,K,V)= softmax(\frac{QK^T}{\sqrt{d_k}})V \end{equation} Here you can think of the left component \(softmax(\frac{QK^T}{\sqrt{d_k}})\) as the matrix of attention weights mentionedin equation (1), where \(V\) is the matrix of values. The above matrix is actually a kind of transformation, and given a new query embedded as \(Q\), that s how you map \(Q\) to the corresponding \(V\). Similarly, if given one single query \(q\), you can arrive at a new value by doing: \begin{equation} Attention_{K,V}(q) = softmax(\frac{qK^T}{\sqrt{d_k}})V \end{equation}</p> <p>In conclusion, with the same set of \(n\) observations, the attention mechanisms allows us to relate one observation to another (and also itself!) via the quasi-similarity matrix \(softmax(\frac{QK^T}{\sqrt{d_k}})\). Most importantly, this matrix is not necessarily symmetric! Therefore, you can think of the attention as a great tool that helps us learn about the correlation/co-occurence pattern between one input observation and another. In biology, co-occurence of input features is a</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Hoang (Anh) Tran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1W7V2Y0MXW"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1W7V2Y0MXW");</script> <script async src="https://rum.cronitor.io/script.js"></script> <script>window.cronitor=window.cronitor||function(){(window.cronitor.q=window.cronitor.q||[]).push(arguments)},cronitor("config",{clientKey:""});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>